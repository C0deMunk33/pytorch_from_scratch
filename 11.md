# Lesson 11: Transformers and Self-Attention**

**1. Introduction to Transformers**

Transformers are a powerful class of neural networks that have achieved state-of-the-art performance in various tasks, particularly in natural language processing (NLP). Introduced by Vaswani et al. in the paper "Attention is All You Need," transformers replace the recurrent and convolutional layers with self-attention mechanisms, enabling efficient parallelization and long-range dependency modeling.

**2. Self-Attention Mechanism**

The self-attention mechanism computes the importance of each input element with respect to every other element in the sequence. It does so by calculating the dot product between the query, key, and value vectors, which are derived from the input embeddings through linear transformations. The result is a weighted sum of the input embeddings, where the weights represent the attention scores.

**3. Multi-head Attention**

Multi-head attention is an extension of the self-attention mechanism that allows the model to focus on different aspects of the input simultaneously. It consists of several parallel self-attention layers, called "heads," each with its own set of learnable parameters. The outputs of these heads are concatenated and passed through a linear layer to obtain the final output.

**4. Positional Encoding**

Since transformers do not have an inherent notion of position or order, positional encoding is used to inject positional information into the input embeddings. This is typically achieved by adding a sinusoidal function of varying frequency to the input embeddings, allowing the model to learn and use the relative positions of the input elements.

**5. Implementing a Transformer for NLP Tasks**

To implement a transformer for NLP tasks in PyTorch, you can use the built-in `nn.Transformer` module, which provides a high-level interface for creating transformer models. Here's a basic example:


```python
import torch
import torch.nn as nn

class TransformerModel(nn.Module):
    def __init__(self, vocab_size, embed_dim, num_heads, num_layers):
        super(TransformerModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.transformer = nn.Transformer(embed_dim, num_heads, num_layers)
        self.fc = nn.Linear(embed_dim, vocab_size)

    def forward(self, x):
        x = self.embedding(x)
        x = self.transformer(x)
        x = self.fc(x)
        return x

vocab_size = ...  # Size of the vocabulary
embed_dim = ...  # Dimension of the input embeddings
num_heads = ...  # Number of attention heads
num_layers = ...  # Number of transformer layers
model = TransformerModel(vocab_size, embed_dim, num_heads, num_layers)
```


**6. Implementing a Vision Transformer for Image Classification**

Vision transformers (ViT) apply the transformer architecture to image classification tasks. They divide an image into non-overlapping patches and linearly embed them into a sequence of vectors, which are then fed into a transformer model. Here's a basic example:

``` python

import torch

import torch.nn as nn

class VisionTransformer(nn.Module):

    def __init__(self, num_classes, embed_dim, num_heads, num_layers, image_size, patch_size):

        super(VisionTransformer, self).__init__()

        self.num_patches = (image_size // patch_size) ** 2

        self.patch_embedding = nn.Linear(patch_size * patch_size * 3, embed_dim)

        self.transformer = nn.Transformer(embed_dim, num_heads, num_layers)

        self.fc = nn.Linear(embed_dim, num_classes)

    def forward(self, x):

        # Reshape the input image into non-overlapping patches and flatten

        x = x.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size).permute(0, 2, 3, 1, 4, 5).contiguous().view(x.size(0), -1, patch_size * patch_size * 3)


        # Embed the patches and pass them through the transformer
        x = self.patch_embedding(x)
        x = self.transformer(x)

        # Average the transformer output across the sequence dimension and pass it through the final classification layer
        x = x.mean(dim=1)
        x = self.fc(x)
        return x



num_classes = ... # Number of classes for the classification task 
embed_dim = ... # Dimension of the input embeddings 
num_heads = ... # Number of attention heads 
num_layers = ... # Number of transformer layers 
image_size = ... # Size of the input images 
patch_size = ... # Size of the patches 

model = VisionTransformer(num_classes, embed_dim, num_heads, num_layers, image_size, patch_size)

```

This example demonstrates a basic implementation of a vision transformer for image classification. Note that this is a simplified version, and more advanced techniques and optimizations can be used in practice.

## Exercises:

1. Implement a simple transformer model for a natural language processing task, such as sentiment analysis or machine translation. Train the model on a suitable dataset and evaluate its performance using appropriate metrics. Compare the results with a baseline model, such as an RNN or CNN-based architecture.
2. Experiment with different configurations of the transformer model by varying the number of layers, attention heads, and embedding dimensions. Analyze the impact of these changes on the model's performance and training time.
3. Implement a vision transformer model for an image classification task, such as CIFAR-10 or ImageNet. Train the model and evaluate its performance using accuracy and other relevant metrics. Compare the results with a baseline convolutional neural network (CNN) model.
4. Explore the concept of transfer learning with transformers by fine-tuning a pre-trained transformer model, such as BERT or GPT, on a downstream task. Evaluate the performance of the fine-tuned model and compare it with a model trained from scratch.
5. Investigate the interpretability of transformer models by visualizing attention weights or using techniques like LIME and SHAP. Analyze the attention patterns and feature importance values to gain insights into the model's decision-making process.