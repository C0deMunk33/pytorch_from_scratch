# Lesson 6: Autoencoders and Variational Autoencoders

**1. Introduction to Autoencoders**

Autoencoders are a type of unsupervised neural network that learns to compress and reconstruct data by minimizing the reconstruction error between the input and output. Autoencoders consist of two parts: an encoder, which maps the input data to a lower-dimensional latent space, and a decoder, which reconstructs the data from the latent space. Autoencoders can be used for dimensionality reduction, denoising, and feature learning.

**2. Implementing an Autoencoder**

Here's an example of a simple autoencoder implementation in PyTorch:

```python
import torch
import torch.nn as nn

class Autoencoder(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super(Autoencoder, self).__init__()


        self.encoder = nn.Sequential(
            nn.Linear(input_dim, latent_dim),
            nn.ReLU(),
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, input_dim),
            nn.Sigmoid(),
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

input_dim = 784  # Example: 28x28 images flattened to a 1D vector
latent_dim = 32  # Size of the latent space
model = Autoencoder(input_dim, latent_dim)
```


In this example, we create a simple autoencoder with a single fully connected layer for both the encoder and the decoder. The input data is expected to be flattened (e.g., 28x28 images as a 1D vector of size 784). You can train the autoencoder using the Mean Squared Error (MSE) loss as the reconstruction error.

**3. Variational Autoencoders (VAEs)**

Variational Autoencoders (VAEs) are an extension of autoencoders that learn a probabilistic representation of the data. In VAEs, the encoder maps the input data to the parameters of a probability distribution in the latent space, while the decoder samples from this distribution to generate the output. VAEs have a regularized training objective, which encourages the model to learn a smooth and structured latent space. VAEs can be used for tasks like data generation, denoising, and representation learning.

**4. Implementing a VAE**

Here's an example of a VAE implementation in PyTorch:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class VAE(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super(VAE, self).__init__()


        self.encoder = nn.Sequential(
            nn.Linear(input_dim, latent_dim * 2),
            nn.ReLU(),
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, input_dim),
            nn.Sigmoid(),
        )

    def reparameterize(self, mu, log_var):
        std = torch.exp(log_var / 2)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        h = self.encoder(x)
        mu, log_var = h.chunk(2, dim=-1)
        z = self.reparameterize(mu, log_var)
        x_reconstructed = self.decoder(z)
        return x_reconstructed, mu, log_var

input_dim = 784
latent_dim = 32
model = VAE(input_dim, latent_dim)
```


In this VAE example, we modify the autoencoder architecture to output two sets of parameters (mean and log-variance) for the latent space distribution. The reparameterization trick is used to sample from this distribution during the forward pass. The decoder remains unchanged compared to the simple autoencoder.

When training a VAE, you need to use a combination of reconstruction loss (e.g., Mean Squared Error) and the Kullback-Leibler (KL) divergence between the learned latent distribution and a prior distribution (usually a standard normal distribution). The KL divergence acts as a regularization term that encourages the learned distribution to be smooth and well-structured.

Here's an example of how to train the VAE using both the reconstruction loss and the KL divergence:


```python
import torch.optim as optim

criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Assuming you have your training data loader named 'train_loader'
num_epochs = 10

for epoch in range(num_epochs):
    model.train()

    for batch_idx, (inputs, _) in enumerate(train_loader):  # Unsupervised learning; we don't need labels
        optimizer.zero_grad()

        inputs = inputs.view(-1, input_dim)  # Flatten the input data
        outputs, mu, log_var = model(inputs)

        # Calculate the reconstruction loss and the KL divergence
        reconstruction_loss = criterion(outputs, inputs)
        kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
        kl_divergence /= inputs.size(0) * input_dim

        # Combine the losses and backpropagate
        loss = reconstruction_loss + kl_divergence
        loss.backward()
        optimizer.step()
```


In this training loop, we calculate the reconstruction loss and KL divergence for each batch and combine them to form the total loss. We then backpropagate the gradients and update the model parameters. Note that the input data should be flattened before feeding it into the model.

## Exercises:



1. Implement a simple autoencoder for an image dataset (e.g., MNIST or CIFAR-10) using PyTorch. Train the autoencoder and analyze the quality of the reconstructed images. Experiment with different architectures and latent space dimensions to observe their effects on the reconstruction quality.
2. Implement a denoising autoencoder by adding noise to the input images and training the model to reconstruct the original (noise-free) images. Analyze the quality of the denoised images and experiment with different noise levels and model architectures.
3. Implement a variational autoencoder (VAE) for an image dataset (e.g., MNIST or CIFAR-10) using PyTorch. Train the VAE and use it to generate new images by sampling from the latent space. Analyze the quality of the generated images and compare them to the ones reconstructed by a simple autoencoder.
4. Train a VAE with different prior distributions (e.g., uniform distribution or mixture of Gaussians) in the latent space. Analyze the impact of different priors on the VAE's generative capabilities and the structure of the latent space.
5. Implement a conditional VAE, which takes both the input data and a label as input, and use it for tasks like controlled image generation or semi-supervised learning. Compare the performance and generative capabilities of the conditional VAE to the vanilla VAE.
6. Experiment with different types of autoencoders, such as contractive autoencoders or sparse autoencoders, and analyze their properties, strengths, and weaknesses in comparison to vanilla autoencoders and VAEs.