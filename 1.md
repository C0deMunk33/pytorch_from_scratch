# Lesson 1: Introduction to PyTorch

**1. What is PyTorch and why use it?**

PyTorch is an open-source machine learning library developed by Facebook's AI Research lab (FAIR). It is primarily used for deep learning and natural language processing applications. PyTorch has gained popularity due to its flexibility, ease of use, and dynamic computation graph, making it an ideal choice for researchers and developers.

Reasons to use PyTorch:



* Dynamic computation graph: PyTorch builds computation graphs on-the-fly, making it easier to debug and visualize.
* Easier to learn and use: PyTorch has a more intuitive and user-friendly API compared to other deep learning frameworks.
* Strong community support: PyTorch has an active community that provides support, pre-trained models, and contributed libraries.
* Research-oriented: PyTorch is designed to support fast experimentation and prototyping, making it popular among researchers.

**2. Installing PyTorch**

To install PyTorch, use the package manager pip or conda. The official PyTorch website ([https://pytorch.org/](https://pytorch.org/)) provides installation instructions for different platforms and configurations. Here's an example command to install PyTorch using pip:


``` bash
pip install torch torchvision -f https://download.pytorch.org/whl/cu102/torch_stable.html
```


Replace `cu102` with your specific CUDA version if you have an NVIDIA GPU. If you don't have a GPU or prefer a CPU-only version, use:



```bash
pip install torch torchvision -f https://download.pytorch.org/whl/cpu/torch_stable.html
```


**3. Understanding Tensors**

Tensors are the fundamental data structure in PyTorch and are similar to NumPy arrays. They can be scalars, vectors, matrices, or multi-dimensional arrays. Tensors are used to represent input data, model weights, and gradients in PyTorch.

To create a tensor, use the `torch.tensor()` function:


``` python
import torch

# Creating a tensor from a list
tensor_a = torch.tensor([1, 2, 3])

# Creating a tensor from a NumPy array
import numpy as np
array = np.array([1, 2, 3])
tensor_b = torch.from_numpy(array)
```


**4. Basic Tensor Operations**

Some basic tensor operations include:



* Element-wise addition, subtraction, multiplication, and division
* Matrix multiplication
* Reshaping tensors
* Indexing and slicing tensors

Here are a few examples:


``` python
# Element-wise operations
tensor_a = torch.tensor([1, 2, 3])
tensor_b = torch.tensor([4, 5, 6])

add_result = tensor_a + tensor_b
sub_result = tensor_a - tensor_b
mul_result = tensor_a * tensor_b
div_result = tensor_a / tensor_b

# Matrix multiplication
matrix_a = torch.tensor([[1, 2], [3, 4]])
matrix_b = torch.tensor([[5, 6], [7, 8]])
matmul_result = torch.matmul(matrix_a, matrix_b)

# Reshaping tensors
tensor = torch.tensor([[1, 2], [3, 4]])
reshaped_tensor = tensor.view(1, -1)

# Indexing and slicing tensors
tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
sliced_tensor = tensor[1:, 1:]
```


**5. Introduction to Autograd**

Autograd is PyTorch's automatic differentiation system. It simplifies computing gradients for optimizing neural network models. When you create a tensor with `requires_grad=True`, PyTorch tracks the operations performed on it. You can then compute gradients using the `backward()` method. Gradients are stored in the `grad` attribute of the tensors.

Here's a simple example demonstrating the usage of Autograd:



``` python
import torch

# Create tensors with requires_grad=True
x = torch.tensor(2.0, requires_grad=True)
y = torch.tensor(3.0, requires_grad=True)

# Perform operations
z = x**2 + y**3

# Compute gradients
z.backward()

# Access the gradients
print('Gradient of z w.r.t x:', x.grad)  # dz/dx = 2 * x = 4
print('Gradient of z w.r.t y:', y.grad)  # dz/dy = 3 * y^2 = 27
```


**Using Autograd for Neural Networks**

In the context of neural networks, Autograd simplifies the computation of gradients for weight updates during backpropagation. Here's a basic example of using Autograd for a simple linear regression model:



```python

import torch

# Generate sample data
x = torch.tensor([[1.0], [2.0], [3.0], [4.0]])
y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])

# Initialize weights and biases
w = torch.tensor(0.0, requires_grad=True)
b = torch.tensor(0.0, requires_grad=True)

# Define model and loss function
def model(x):
    return w * x + b

def mse_loss(y_pred, y_true):
    return ((y_pred - y_true)**2).mean()

# Training loop
learning_rate = 0.01
epochs = 100

for epoch in range(epochs):
    # Forward pass
    y_pred = model(x)


    # Compute loss
    loss = mse_loss(y_pred, y)

    # Compute gradients
    loss.backward()

    # Update weights and biases
    with torch.no_grad():
        w -= learning_rate * w.grad
        b -= learning_rate * b.grad


    # Zero gradients
    w.grad.zero_()
    b.grad.zero_()

    # Print progress
    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item()}')
```


In this example, we use Autograd to compute gradients for the weight `w` and bias `b` of the linear regression model. The gradients are then used to update the model parameters during the training loop.


## **Exercises**

1. Install PyTorch on your local machine or set up a virtual environment in the cloud (e.g., using Google Colab or AWS).
2. Create the following tensors using PyTorch: a. A 2x3 matrix filled with ones. b. A 4x4 matrix filled with random values. c. A 5x5 identity matrix.
3. Perform the following tensor operations using PyTorch: a. Add two tensors of the same shape. b. Multiply two tensors element-wise. c. Perform matrix multiplication between two tensors. d. Calculate the mean and standard deviation of a tensor.
4. Create a simple computational graph using PyTorch, and demonstrate how the autograd system computes gradients. For example, define a simple function, such as `f(x) = x^2 + 3x + 5`, and calculate its gradient with respect to `x` at different points.
5. Modify the computational graph you created in Exercise 4 by introducing an intermediate variable. Calculate the gradient of the output with respect to both the input and the intermediate variable. For example, define a function `g(x) = x^2`, and then define `f(x) = g(x) + 3x + 5`. Compute the gradients with respect to `x` and `g(x)`.