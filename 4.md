# Lesson 4: Recurrent Neural Networks (RNNs)

**1. Introduction to RNNs**

Recurrent Neural Networks (RNNs) are a type of neural network designed for processing sequences of data. Unlike feedforward neural networks, RNNs have connections between neurons that form loops, allowing them to maintain a hidden state that can capture information from previous time steps. RNNs are widely used for natural language processing, time series analysis, and other sequence-based tasks.

**2. LSTM and GRU**

One of the main challenges with RNNs is the vanishing gradient problem, which makes it difficult for them to learn long-range dependencies in the data. Two popular variants of RNNs that address this issue are Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks.



* LSTM: LSTMs have a more complex structure than standard RNNs, consisting of a memory cell and three gates (input, output, and forget) that control the flow of information. LSTMs can learn to store and manipulate long-range dependencies in the data more effectively than vanilla RNNs.
* GRU: GRUs are a simpler version of LSTMs with two gates (reset and update) instead of three. They offer a good balance between complexity and learning capability, often achieving similar performance to LSTMs with fewer parameters.

**3. Implementing RNNs for Text Classification**

Here's an example of implementing an LSTM-based RNN for text classification using PyTorch:


```python
import torch
import torch.nn as nn
import torch.optim as optim

class TextClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):
        super(TextClassifier, self).__init__()


        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, num_classes)

    def forward(self, x):
        x = self.embedding(x)
        _, (hidden, _) = self.lstm(x)
        x = hidden[-1]
        x = self.fc(x)
        return x

# Example usage:
vocab_size = 10000
embedding_dim = 128
hidden_dim = 256
num_classes = 2
model = TextClassifier(vocab_size, embedding_dim, hidden_dim, num_classes)
input_data = torch.randint(0, vocab_size, (32, 50))  # Batch of 32 sequences, each of length 50
output = model(input_data)
```


In this example, we first convert the input sequences of token indices into word embeddings using an `nn.Embedding` layer. Then, we pass the embeddings through an LSTM layer to capture the sequential information. Finally, we use a fully connected layer to map the LSTM's hidden state to the output classes.

**4. Generating Text with RNNs**

Text generation with RNNs involves training the network to predict the next token in a sequence given the previous tokens. After training, you can generate new sequences by sampling tokens from the model's predictions and feeding them back as input. Here's an example of implementing text generation with an LSTM-based RNN:

```python

class TextGenerator(nn.Module):

    def __init__(self, vocab_size, embedding_dim, hidden_dim):

        super(TextGenerator, self).__init__()

        

        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)

        self.fc = nn.Linear(hidden_dim, vocab_size)

    def forward(self, x, hidden):

        x = self`.embedding(x)`
        x, hidden = self.lstm(x, hidden)
        x = self.fc(x)
        return x, hidden

# Example usage:
vocab_size = 10000
embedding_dim = 128
hidden_dim = 256
model = TextGenerator(vocab_size, embedding_dim, hidden_dim)

# Assuming you have your training data loader named 'train_loader'
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

num_epochs = 10

for epoch in range(num_epochs):
    model.train()


    # Training loop
    for batch_idx, (inputs, targets) in enumerate(train_loader):
        hidden = None
        optimizer.zero_grad()


        loss = 0
        for t in range(inputs.size(1) - 1):  # Iterate through each token in the sequence
            outputs, hidden = model(inputs[:, t].unsqueeze(1), hidden)
            loss += criterion(outputs.squeeze(1), targets[:, t + 1])

        loss /= inputs.size(1) - 1
        loss.backward()
        optimizer.step()

# Text generation (sampling)
model.eval()

start_token = torch.tensor([0])  # Assuming 0 is the index for the start token
input_data = start_token.unsqueeze(0)
generated_sequence = [start_token.item()]
hidden = None

for _ in range(100):  # Generate a sequence of length 100
    with torch.no_grad():
        output, hidden = model(input_data, hidden)
        probabilities = torch.softmax(output, dim=-1)
        token = torch.multinomial(probabilities, 1).item()
        generated_sequence.append(token)
        input_data = torch.tensor([token]).unsqueeze(0)

print("Generated sequence:", generated_sequence)
```


In this example, we extend the previous text classifier architecture to perform text generation. The main difference is that the model now outputs a probability distribution over the vocabulary for each token in the sequence. During training, we minimize the cross-entropy loss between the model's predictions and the true next tokens.

For text generation, we start with a start token and iteratively sample tokens from the model's predictions, feeding them back as input. The generated tokens form the new sequence.

## Exercises:



1. Implement an RNN for sentiment analysis using PyTorch on a real-world dataset, such as the IMDb movie reviews dataset. Experiment with different architectures (vanilla RNN, LSTM, GRU) and hyperparameters to improve the model's performance.
2. Implement a character-level RNN for text generation using PyTorch. Train the model on a large corpus of text, such as a book or a collection of articles, and generate new text samples. Analyze the quality of the generated text and experiment with different model architectures and hyperparameters.
3. Extend the text classification example to perform sequence labeling, such as part-of-speech tagging or named entity recognition. Modify the model to output a label for each token in the input sequence and train it on a suitable dataset.
4. Implement a bidirectional RNN for text classification or sequence labeling using PyTorch. Compare the performance of the bidirectional model with a unidirectional model and analyze the benefits and challenges of using bidirectional RNNs.
5. Experiment with different methods for combining the hidden states of an RNN for text classification, such as max-pooling, average pooling, or attention mechanisms. Analyze the impact of these methods on the model's performance.
6. Implement a seq2seq model using RNNs for a machine translation task. Train the model on a parallel corpus of source and target language sentences and evaluate its performance in terms of translation quality. Experiment with different model architectures, such as using attention mechanisms, to improve the model's performance.