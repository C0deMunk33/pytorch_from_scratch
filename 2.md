# Lesson 2: Deep Learning Basics

**1. Introduction to Neural Networks**

Neural networks are a class of machine learning algorithms inspired by the human brain. They consist of interconnected nodes or "neurons" organized in layers. Each neuron receives input from previous layers, processes it, and passes the output to the next layer. The power of neural networks comes from their ability to learn complex patterns in the data through training.

**2. Feedforward Neural Networks**

A feedforward neural network (FNN) is a type of neural network in which the connections between neurons are unidirectional, meaning data flows in one direction from the input layer through hidden layers to the output layer. FNNs are the simplest form of neural networks and are widely used for various tasks, such as regression and classification.

**3. Activation Functions**

Activation functions introduce non-linearity into neural networks, allowing them to learn complex, non-linear relationships in the data. Some common activation functions include:



* Sigmoid: `f(x) = 1 / (1 + exp(-x))`
* Hyperbolic tangent (tanh): `f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))`
* Rectified Linear Unit (ReLU): `f(x) = max(0, x)`
* Leaky ReLU: `f(x) = max(alpha * x, x)`, where `alpha` is a small constant (e.g., 0.01)

**4. Loss Functions**

Loss functions quantify the difference between the predicted output and the true target values. They guide the learning process by providing feedback on the model's performance. Some common loss functions include:



* Mean Squared Error (MSE): Used for regression tasks, it calculates the average squared difference between predicted and true values.
* Cross-Entropy Loss: Used for classification tasks, it measures the dissimilarity between predicted probabilities and true labels.
* Hinge Loss: Used for support vector machines and some neural network classifiers, it measures the error for classification tasks.

**5. Optimizers and Backpropagation**

Optimizers are algorithms that adjust the model's parameters (weights and biases) to minimize the loss function. Backpropagation is the core algorithm behind training neural networks. It calculates the gradients of the loss function with respect to each parameter by applying the chain rule of calculus. Some common optimizers include:



* Stochastic Gradient Descent (SGD): The most basic optimizer, it updates the parameters based on the gradients multiplied by a learning rate.
* Momentum: An extension of SGD, it adds a momentum term to the parameter update to improve convergence.
* RMSprop: Adapts the learning rate for each parameter by dividing the gradient by an exponentially weighted moving average of the squared gradients.
* Adam: Combines the ideas of Momentum and RMSprop, it uses adaptive learning rates and momentum for faster convergence.

**6. Training and Validation**

Training a neural network involves iterating through the following steps:



1. Forward pass: Compute the output of the network by passing the input data through each layer.
2. Calculate loss: Compute the loss function based on the network output and true target values.
3. Backward pass: Calculate gradients of the loss function with respect to model parameters using backpropagation.
4. Update parameters: Use an optimizer to update the model parameters based on the computed gradients.

During training, it's essential to track the model's performance on a separate validation set to prevent overfitting. Overfitting occurs when the model learns to perform well on the training data but fails to generalize to new, unseen data. Techniques like early stopping and regularization can help mitigate overfitting.

## **Exercises:**

1. Implement a simple feedforward neural network using PyTorch for a binary classification problem. Use a toy dataset, such as the XOR problem or a synthetic dataset generated using scikit-learn.
2. Experiment with different activation functions in your neural network implementation from Exercise 1. Compare the performance of the network with Sigmoid, ReLU, and tanh activation functions.
3. Implement a multi-class classification problem using a feedforward neural network in PyTorch. Use a real-world dataset, such as the Iris or MNIST dataset.
4. Train your neural network from Exercise 3 using different optimizers (SGD, Momentum, RMSprop, and Adam). Compare their performance in terms of training time and validation accuracy.
5. Implement a regularization technique, such as L1 or L2 regularization or dropout, in your neural network from Exercise 3. Observe the effect of regularization on the model's performance and overfitting.
6. Use cross-validation to tune the hyperparameters of your neural network, such as the number of hidden layers, the number of neurons per layer, learning rate, and batch size. Analyze the impact of these hyperparameters on the model's performance.